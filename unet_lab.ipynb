{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvc clinic DB\n",
      "entire dataset length:  612\n",
      "entire train dataset length:  367\n",
      "entire val dataset length:  122\n",
      "entire test dataset length:  123\n",
      "\n",
      "Kvasir SEG\n",
      "entire dataset length:  1000\n",
      "entire train dataset length:  600\n",
      "entire val dataset length:  200\n",
      "entire test dataset length:  200\n",
      "\n",
      "Kvasir instrument\n",
      "entire dataset length:  590\n",
      "entire train dataset length:  354\n",
      "entire val dataset length:  118\n",
      "entire test dataset length:  118\n",
      "\n",
      "ISIC 2017\n",
      "entire dataset length:  2000\n",
      "entire train dataset length:  1200\n",
      "entire val dataset length:  400\n",
      "entire test dataset length:  400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image  \n",
    "import os\n",
    "from torchvision.transforms import functional as TF\n",
    "import random\n",
    "\n",
    "# transform 정의\n",
    "class ImageMaskTransform:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.resize = transforms.Resize((384, 384))\n",
    "        self.rotation = transforms.RandomRotation(degrees=30)\n",
    "        self.h_flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "        self.v_flip = transforms.RandomVerticalFlip(p=0.5)\n",
    "        self.affine = transforms.RandomAffine(degrees=0, scale=(0.8, 1.2), translate=(0.1, 0.1))\n",
    "        self.color_jitter = transforms.ColorJitter(brightness=0.4)\n",
    "        self.crop = transforms.RandomResizedCrop(size=(384, 384))\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "\n",
    "        angle = random.uniform(-5, 5)\n",
    "        image = TF.rotate(image, angle)\n",
    "        mask = TF.rotate(mask, angle)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "\n",
    "        angle, translations, scale, shear = self.affine.get_params(self.affine.degrees, self.affine.translate, self.affine.scale, self.affine.shear, image.size)\n",
    "        image = TF.affine(image, angle, translations, scale, shear)\n",
    "        mask = TF.affine(mask, angle, translations, scale, shear)\n",
    "\n",
    "        image = self.color_jitter(image)\n",
    "\n",
    "        i, j, h, w = transforms.RandomResizedCrop.get_params(image, scale=(0.8, 1.0), ratio=(0.75, 1.33))\n",
    "        image = TF.resized_crop(image, i, j, h, w, size=(384, 384))\n",
    "        mask = TF.resized_crop(mask, i, j, h, w, size=(384, 384))\n",
    "\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")  \n",
    "        mask = Image.open(mask_path).convert(\"L\")  \n",
    "        \n",
    "        if self.transform:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# load, split dataset with paper setting\n",
    "def load_and_split_dataset(image_dir, mask_dir, batch_size=10, name = None):\n",
    "\n",
    "    valid_extensions = ('.tif', '.tiff', '.jpg', '.jpeg', '.png')\n",
    "    image_paths = sorted([os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname.lower().endswith(valid_extensions)])\n",
    "    mask_paths = sorted([os.path.join(mask_dir, fname) for fname in os.listdir(mask_dir) if fname.lower().endswith(valid_extensions)])\n",
    "\n",
    "    dataset = SegmentationDataset(image_paths, mask_paths, transform=ImageMaskTransform())\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    print(name)\n",
    "    print(\"entire dataset length: \", len(train_dataset) + len(test_dataset))\n",
    "    \n",
    "    val_size = int(0.25 * train_size)\n",
    "    train_size = train_size - val_size\n",
    "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "    # dataset count check\n",
    "    print(\"entire train dataset length: \", len(train_dataset))\n",
    "    print(\"entire val dataset length: \", len(val_dataset))\n",
    "    print(\"entire test dataset length: \", len(test_dataset))\n",
    "    print()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Load datasets\n",
    "cvc_train_loader, cvc_val_loader, cvc_test_loader = load_and_split_dataset(\n",
    "    image_dir='CVC-ClinicDB/train', \n",
    "    mask_dir='CVC-ClinicDB/masks',\n",
    "    name = \"cvc clinic DB\"\n",
    ")\n",
    "\n",
    "kvasirseg_train_loader, kvasirseg_val_loader, kvasirseg_test_loader = load_and_split_dataset(\n",
    "    image_dir='/Kvasir-SEG/images', \n",
    "    mask_dir='/Kvasir-SEG/masks',\n",
    "    name = 'Kvasir SEG'\n",
    ")\n",
    "\n",
    "kvasirinst_train_loader, kvasirinst_val_loader, kvasirinst_test_loader = load_and_split_dataset(\n",
    "    image_dir='kvasir-instrument/images/images', \n",
    "    mask_dir='kvasir-instrument/mask',\n",
    "    name = 'Kvasir instrument'\n",
    ")\n",
    "\n",
    "isic_train_loader, isic_val_loader, isic_test_loader = load_and_split_dataset(\n",
    "    image_dir='ISIC-2017_Training_Data', \n",
    "    mask_dir='/ISIC-2017_Training_GroundTruth',\n",
    "    name = 'ISIC 2017'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Conv Layer는 여러 번 쓰이므로 하나의 class로 정의\n",
    "class conv_layer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True):\n",
    "        super(conv_layer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(U_Net, self).__init__()\n",
    "\n",
    "        # Contracting path\n",
    "        self.enc1_1 = conv_layer(in_channels=3, out_channels=64)\n",
    "        self.enc1_2 = conv_layer(in_channels=64, out_channels=64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc2_1 = conv_layer(in_channels=64, out_channels=128)\n",
    "        self.enc2_2 = conv_layer(in_channels=128, out_channels=128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc3_1 = conv_layer(in_channels=128, out_channels=256)\n",
    "        self.enc3_2 = conv_layer(in_channels=256, out_channels=256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc4_1 = conv_layer(in_channels=256, out_channels=512)\n",
    "        self.enc4_2 = conv_layer(in_channels=512, out_channels=512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.enc5_1 = conv_layer(in_channels=512, out_channels=1024)\n",
    "\n",
    "        # Expansive path\n",
    "        self.dec5_1 = conv_layer(in_channels=1024, out_channels=512)\n",
    "        self.unpool4 = nn.ConvTranspose2d(in_channels=512, out_channels=512,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec4_2 = conv_layer(in_channels=2 * 512, out_channels=512)\n",
    "        self.dec4_1 = conv_layer(in_channels=512, out_channels=256)\n",
    "        self.unpool3 = nn.ConvTranspose2d(in_channels=256, out_channels=256,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec3_2 = conv_layer(in_channels=2 * 256, out_channels=256)\n",
    "        self.dec3_1 = conv_layer(in_channels=256, out_channels=128)\n",
    "        self.unpool2 = nn.ConvTranspose2d(in_channels=128, out_channels=128,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec2_2 = conv_layer(in_channels=2 * 128, out_channels=128)\n",
    "        self.dec2_1 = conv_layer(in_channels=128, out_channels=64)\n",
    "        self.unpool1 = nn.ConvTranspose2d(in_channels=64, out_channels=64,\n",
    "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
    "\n",
    "        self.dec1_2 = conv_layer(in_channels=2 * 64, out_channels=64)\n",
    "        self.dec1_1 = conv_layer(in_channels=64, out_channels=64)\n",
    "        self.fc = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1_1 = self.enc1_1(x)\n",
    "        enc1_2 = self.enc1_2(enc1_1)\n",
    "        pool1 = self.pool1(enc1_2)\n",
    "\n",
    "        enc2_1 = self.enc2_1(pool1)\n",
    "        enc2_2 = self.enc2_2(enc2_1)\n",
    "        pool2 = self.pool2(enc2_2)\n",
    "\n",
    "        enc3_1 = self.enc3_1(pool2)\n",
    "        enc3_2 = self.enc3_2(enc3_1)\n",
    "        pool3 = self.pool3(enc3_2)\n",
    "\n",
    "        enc4_1 = self.enc4_1(pool3)\n",
    "        enc4_2 = self.enc4_2(enc4_1)\n",
    "        pool4 = self.pool4(enc4_2)\n",
    "\n",
    "        enc5_1 = self.enc5_1(pool4)\n",
    "\n",
    "        dec5_1 = self.dec5_1(enc5_1)\n",
    "\n",
    "        unpool4 = self.unpool4(dec5_1)\n",
    "        cat4 = torch.cat((unpool4, enc4_2), dim=1) # Skip Connection\n",
    "        dec4_2 = self.dec4_2(cat4)\n",
    "        dec4_1 = self.dec4_1(dec4_2)\n",
    "\n",
    "        unpool3 = self.unpool3(dec4_1)\n",
    "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
    "        dec3_2 = self.dec3_2(cat3)\n",
    "        dec3_1 = self.dec3_1(dec3_2)\n",
    "\n",
    "        unpool2 = self.unpool2(dec3_1)\n",
    "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
    "        dec2_2 = self.dec2_2(cat2)\n",
    "        dec2_1 = self.dec2_1(dec2_2)\n",
    "\n",
    "        unpool1 = self.unpool1(dec2_1)\n",
    "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
    "        dec1_2 = self.dec1_2(cat1)\n",
    "        dec1_1 = self.dec1_1(dec1_2)\n",
    "\n",
    "        x = self.fc(dec1_1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameter setting\n",
    "lr = 1e-4\n",
    "patience = 10\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "num_epochs = 500\n",
    "\n",
    "#train - early stopping + get best val loss\n",
    "def train_model(model, train_loader, val_loader, num_epochs=num_epochs):\n",
    "    best_loss = float('inf')  \n",
    "    epochs_no_improve = 0  \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  \n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = validate_model(model, val_loader)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            print('best loss: ',best_loss)\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0 \n",
    "        else:\n",
    "            epochs_no_improve += 1  \n",
    "            \n",
    "        if epochs_no_improve > patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break  \n",
    "\n",
    "        if best_model_state is not None:\n",
    "            model.load_state_dict(best_model_state)\n",
    "\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  \n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    return val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# batch 당 iou 정의, threshold = 0.5\n",
    "def calculate_batch_iou(predictions, targets, threshold=0.5, eps = 1e-8):\n",
    "\n",
    "    batch_size = predictions.size(0)\n",
    "    predictions = predictions.float()  \n",
    "    \n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    \n",
    "    pred_class = (predictions >= threshold).float()\n",
    "    \n",
    "    pred_class = pred_class.view(batch_size, -1)\n",
    "    target_class = targets.view(batch_size, -1).float()\n",
    "    \n",
    "    intersection = (pred_class * target_class).sum(dim=1)\n",
    "    union = pred_class.sum(dim=1) + target_class.sum(dim=1) - intersection\n",
    "    \n",
    "    iou = (intersection +eps) / (union+eps) \n",
    "    \n",
    "    return iou.mean().item()\n",
    "\n",
    "#AUROC 정의 - sklearn package 사용\n",
    "def compute_auroc(pred_mask, true_mask):\n",
    "    \n",
    "    pred_flat = pred_mask.view(-1).cpu().numpy()\n",
    "    true_flat = true_mask.view(-1).cpu().numpy().astype(int)\n",
    "    auroc = roc_auc_score(true_flat, pred_flat)\n",
    "\n",
    "    return auroc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def inference(model, test_loader):\n",
    "    model.eval()  \n",
    "    auroc_score = 0.0\n",
    "    total_samples = 0\n",
    "    class_iou = np.zeros(1)\n",
    "    threshold = 0.5\n",
    "    total_batch = 0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, targets in test_loader:\n",
    "\n",
    "            images = images.to(device)\n",
    "            targets_iou = targets.to(device)\n",
    "\n",
    "            targets = targets_iou.squeeze(1).long()\n",
    "            y_pred = model(images)\n",
    "\n",
    "            batch_iou = calculate_batch_iou(y_pred, targets_iou, threshold=threshold)\n",
    "            class_iou += batch_iou \n",
    "            total_batch +=1\n",
    "            \n",
    "            batch_auroc = compute_auroc(y_pred, targets)   \n",
    "            auroc_score += batch_auroc * images.size(0) \n",
    "            total_samples += images.size(0)\n",
    "\n",
    "    avg_auroc = auroc_score / total_samples\n",
    "    avg_iou = class_iou / total_batch\n",
    "\n",
    "    print(f\"Average IoU: {avg_iou[0]:.4f}\")\n",
    "    print(f\"Average AUROC: {avg_auroc:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Training Loss: 0.5879 - Val Loss: 0.4753\n",
      "best loss:  0.47529339301781576\n",
      "Epoch [2/500] - Training Loss: 0.4430 - Val Loss: 0.4143\n",
      "best loss:  0.41433037695337516\n",
      "Epoch [3/500] - Training Loss: 0.4136 - Val Loss: 0.4104\n",
      "best loss:  0.41040347247827247\n",
      "Epoch [4/500] - Training Loss: 0.3904 - Val Loss: 0.3951\n",
      "best loss:  0.3951438640961882\n",
      "Epoch [5/500] - Training Loss: 0.3762 - Val Loss: 0.3754\n",
      "best loss:  0.3754367100410774\n",
      "Epoch [6/500] - Training Loss: 0.3647 - Val Loss: 0.3571\n",
      "best loss:  0.3571133618472052\n",
      "Epoch [7/500] - Training Loss: 0.3504 - Val Loss: 0.3583\n",
      "Epoch [8/500] - Training Loss: 0.3384 - Val Loss: 0.3388\n",
      "best loss:  0.3387653744611584\n",
      "Epoch [9/500] - Training Loss: 0.3308 - Val Loss: 0.3349\n",
      "best loss:  0.3348921869621902\n",
      "Epoch [10/500] - Training Loss: 0.3164 - Val Loss: 0.3146\n",
      "best loss:  0.31462047334577214\n",
      "Epoch [11/500] - Training Loss: 0.3084 - Val Loss: 0.3406\n",
      "Epoch [12/500] - Training Loss: 0.2958 - Val Loss: 0.3179\n",
      "Epoch [13/500] - Training Loss: 0.2935 - Val Loss: 0.2933\n",
      "best loss:  0.29329389236012443\n",
      "Epoch [14/500] - Training Loss: 0.2843 - Val Loss: 0.3057\n",
      "Epoch [15/500] - Training Loss: 0.2795 - Val Loss: 0.2833\n",
      "best loss:  0.28331793331709065\n",
      "Epoch [16/500] - Training Loss: 0.2674 - Val Loss: 0.2999\n",
      "Epoch [17/500] - Training Loss: 0.2610 - Val Loss: 0.2771\n",
      "best loss:  0.2771260943569121\n",
      "Epoch [18/500] - Training Loss: 0.2554 - Val Loss: 0.2571\n",
      "best loss:  0.25714523821580604\n",
      "Epoch [19/500] - Training Loss: 0.2483 - Val Loss: 0.2743\n",
      "Epoch [20/500] - Training Loss: 0.2548 - Val Loss: 0.2602\n",
      "Epoch [21/500] - Training Loss: 0.2361 - Val Loss: 0.2510\n",
      "best loss:  0.250962959205518\n",
      "Epoch [22/500] - Training Loss: 0.2339 - Val Loss: 0.2517\n",
      "Epoch [23/500] - Training Loss: 0.2313 - Val Loss: 0.2574\n",
      "Epoch [24/500] - Training Loss: 0.2240 - Val Loss: 0.2447\n",
      "best loss:  0.24467056256825806\n",
      "Epoch [25/500] - Training Loss: 0.2169 - Val Loss: 0.2377\n",
      "best loss:  0.23768018552514372\n",
      "Epoch [26/500] - Training Loss: 0.2119 - Val Loss: 0.2316\n",
      "best loss:  0.23155295824418304\n",
      "Epoch [27/500] - Training Loss: 0.2137 - Val Loss: 0.2278\n",
      "best loss:  0.22779547630763444\n",
      "Epoch [28/500] - Training Loss: 0.2054 - Val Loss: 0.2350\n",
      "Epoch [29/500] - Training Loss: 0.2019 - Val Loss: 0.2204\n",
      "best loss:  0.22040624965409764\n",
      "Epoch [30/500] - Training Loss: 0.2036 - Val Loss: 0.2279\n",
      "Epoch [31/500] - Training Loss: 0.1945 - Val Loss: 0.2146\n",
      "best loss:  0.21461381018161774\n",
      "Epoch [32/500] - Training Loss: 0.1860 - Val Loss: 0.2078\n",
      "best loss:  0.2078053506671405\n",
      "Epoch [33/500] - Training Loss: 0.1855 - Val Loss: 0.2018\n",
      "best loss:  0.20176148023761686\n",
      "Epoch [34/500] - Training Loss: 0.1779 - Val Loss: 0.2034\n",
      "Epoch [35/500] - Training Loss: 0.1786 - Val Loss: 0.2085\n",
      "Epoch [36/500] - Training Loss: 0.1743 - Val Loss: 0.2242\n",
      "Epoch [37/500] - Training Loss: 0.1703 - Val Loss: 0.1971\n",
      "best loss:  0.19705956807879152\n",
      "Epoch [38/500] - Training Loss: 0.1692 - Val Loss: 0.1924\n",
      "best loss:  0.19235365373677896\n",
      "Epoch [39/500] - Training Loss: 0.1684 - Val Loss: 0.1998\n",
      "Epoch [40/500] - Training Loss: 0.1602 - Val Loss: 0.1809\n",
      "best loss:  0.1808622584479754\n",
      "Epoch [41/500] - Training Loss: 0.1656 - Val Loss: 0.1815\n",
      "Epoch [42/500] - Training Loss: 0.1615 - Val Loss: 0.2059\n",
      "Epoch [43/500] - Training Loss: 0.1603 - Val Loss: 0.1799\n",
      "best loss:  0.17994201354316022\n",
      "Epoch [44/500] - Training Loss: 0.1594 - Val Loss: 0.1919\n",
      "Epoch [45/500] - Training Loss: 0.1459 - Val Loss: 0.1626\n",
      "best loss:  0.16262533869899687\n",
      "Epoch [46/500] - Training Loss: 0.1376 - Val Loss: 0.1710\n",
      "Epoch [47/500] - Training Loss: 0.1362 - Val Loss: 0.1965\n",
      "Epoch [48/500] - Training Loss: 0.1445 - Val Loss: 0.1670\n",
      "Epoch [49/500] - Training Loss: 0.1292 - Val Loss: 0.1639\n",
      "Epoch [50/500] - Training Loss: 0.1329 - Val Loss: 0.1551\n",
      "best loss:  0.15512455438004166\n",
      "Epoch [51/500] - Training Loss: 0.1312 - Val Loss: 0.1409\n",
      "best loss:  0.14090984009328436\n",
      "Epoch [52/500] - Training Loss: 0.1278 - Val Loss: 0.1735\n",
      "Epoch [53/500] - Training Loss: 0.1175 - Val Loss: 0.1714\n",
      "Epoch [54/500] - Training Loss: 0.1249 - Val Loss: 0.1591\n",
      "Epoch [55/500] - Training Loss: 0.1173 - Val Loss: 0.1537\n",
      "Epoch [56/500] - Training Loss: 0.1203 - Val Loss: 0.1609\n",
      "Epoch [57/500] - Training Loss: 0.1101 - Val Loss: 0.1579\n",
      "Epoch [58/500] - Training Loss: 0.1147 - Val Loss: 0.1439\n",
      "Epoch [59/500] - Training Loss: 0.1150 - Val Loss: 0.1543\n",
      "Epoch [60/500] - Training Loss: 0.1084 - Val Loss: 0.1423\n",
      "Epoch [61/500] - Training Loss: 0.1108 - Val Loss: 0.1416\n",
      "Epoch [62/500] - Training Loss: 0.1014 - Val Loss: 0.1271\n",
      "best loss:  0.1271103527702269\n",
      "Epoch [63/500] - Training Loss: 0.1027 - Val Loss: 0.1258\n",
      "best loss:  0.1258077244656008\n",
      "Epoch [64/500] - Training Loss: 0.0971 - Val Loss: 0.1257\n",
      "best loss:  0.12572682868750368\n",
      "Epoch [65/500] - Training Loss: 0.0950 - Val Loss: 0.1397\n",
      "Epoch [66/500] - Training Loss: 0.0972 - Val Loss: 0.1347\n",
      "Epoch [67/500] - Training Loss: 0.0924 - Val Loss: 0.1214\n",
      "best loss:  0.12140729247790868\n",
      "Epoch [68/500] - Training Loss: 0.0990 - Val Loss: 0.1538\n",
      "Epoch [69/500] - Training Loss: 0.1020 - Val Loss: 0.1175\n",
      "best loss:  0.11753011965116517\n",
      "Epoch [70/500] - Training Loss: 0.1027 - Val Loss: 0.1141\n",
      "best loss:  0.11405414855871045\n",
      "Epoch [71/500] - Training Loss: 0.0924 - Val Loss: 0.1236\n",
      "Epoch [72/500] - Training Loss: 0.0871 - Val Loss: 0.1162\n",
      "Epoch [73/500] - Training Loss: 0.0874 - Val Loss: 0.1197\n",
      "Epoch [74/500] - Training Loss: 0.0860 - Val Loss: 0.1349\n",
      "Epoch [75/500] - Training Loss: 0.0880 - Val Loss: 0.1094\n",
      "best loss:  0.10942186396874365\n",
      "Epoch [76/500] - Training Loss: 0.0970 - Val Loss: 0.1171\n",
      "Epoch [77/500] - Training Loss: 0.0883 - Val Loss: 0.1268\n",
      "Epoch [78/500] - Training Loss: 0.0788 - Val Loss: 0.1254\n",
      "Epoch [79/500] - Training Loss: 0.0779 - Val Loss: 0.1323\n",
      "Epoch [80/500] - Training Loss: 0.0929 - Val Loss: 0.1275\n",
      "Epoch [81/500] - Training Loss: 0.0818 - Val Loss: 0.1101\n",
      "Epoch [82/500] - Training Loss: 0.0752 - Val Loss: 0.1230\n",
      "Epoch [83/500] - Training Loss: 0.0749 - Val Loss: 0.1152\n",
      "Epoch [84/500] - Training Loss: 0.0754 - Val Loss: 0.1254\n",
      "Epoch [85/500] - Training Loss: 0.0719 - Val Loss: 0.1039\n",
      "best loss:  0.1039048463228296\n",
      "Epoch [86/500] - Training Loss: 0.0806 - Val Loss: 0.1131\n",
      "Epoch [87/500] - Training Loss: 0.0734 - Val Loss: 0.1197\n",
      "Epoch [88/500] - Training Loss: 0.0772 - Val Loss: 0.1063\n",
      "Epoch [89/500] - Training Loss: 0.0680 - Val Loss: 0.1110\n",
      "Epoch [90/500] - Training Loss: 0.0679 - Val Loss: 0.1162\n",
      "Epoch [91/500] - Training Loss: 0.0694 - Val Loss: 0.1206\n",
      "Epoch [92/500] - Training Loss: 0.0677 - Val Loss: 0.1033\n",
      "best loss:  0.10334905872090919\n",
      "Epoch [93/500] - Training Loss: 0.0742 - Val Loss: 0.1414\n",
      "Epoch [94/500] - Training Loss: 0.0728 - Val Loss: 0.1242\n",
      "Epoch [95/500] - Training Loss: 0.0614 - Val Loss: 0.1124\n",
      "Epoch [96/500] - Training Loss: 0.0780 - Val Loss: 0.1184\n",
      "Epoch [97/500] - Training Loss: 0.0654 - Val Loss: 0.1291\n",
      "Epoch [98/500] - Training Loss: 0.0598 - Val Loss: 0.1069\n",
      "Epoch [99/500] - Training Loss: 0.0652 - Val Loss: 0.1248\n",
      "Epoch [100/500] - Training Loss: 0.0676 - Val Loss: 0.1223\n",
      "Epoch [101/500] - Training Loss: 0.0675 - Val Loss: 0.1287\n",
      "Epoch [102/500] - Training Loss: 0.0674 - Val Loss: 0.1184\n",
      "Epoch [103/500] - Training Loss: 0.0620 - Val Loss: 0.1050\n",
      "Early stopping triggered at epoch 103\n"
     ]
    }
   ],
   "source": [
    "model = U_Net()\n",
    "model = model.to(device)\n",
    "train_model(model, cvc_train_loader, cvc_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.7232\n",
      "Average AUROC: 0.9811\n"
     ]
    }
   ],
   "source": [
    "inference(model, cvc_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment U-NET backbone: vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Training Loss: 0.5575 - Val Loss: 0.4140\n",
      "best loss:  0.4140317166437868\n",
      "Epoch [2/500] - Training Loss: 0.3789 - Val Loss: 0.3313\n",
      "best loss:  0.3313110568484322\n",
      "Epoch [3/500] - Training Loss: 0.2884 - Val Loss: 0.2521\n",
      "best loss:  0.2521477896170538\n",
      "Epoch [4/500] - Training Loss: 0.2266 - Val Loss: 0.2572\n",
      "Epoch [5/500] - Training Loss: 0.2063 - Val Loss: 0.2106\n",
      "best loss:  0.2106130267264413\n",
      "Epoch [6/500] - Training Loss: 0.1844 - Val Loss: 0.1795\n",
      "best loss:  0.17952794593865753\n",
      "Epoch [7/500] - Training Loss: 0.1673 - Val Loss: 0.1721\n",
      "best loss:  0.17212115020536986\n",
      "Epoch [8/500] - Training Loss: 0.1515 - Val Loss: 0.1637\n",
      "best loss:  0.16368669955456844\n",
      "Epoch [9/500] - Training Loss: 0.1414 - Val Loss: 0.1530\n",
      "best loss:  0.15296817399927828\n",
      "Epoch [10/500] - Training Loss: 0.1274 - Val Loss: 0.1447\n",
      "best loss:  0.14471675650995286\n",
      "Epoch [11/500] - Training Loss: 0.1202 - Val Loss: 0.1327\n",
      "best loss:  0.1326880401275197\n",
      "Epoch [12/500] - Training Loss: 0.1118 - Val Loss: 0.1500\n",
      "Epoch [13/500] - Training Loss: 0.1004 - Val Loss: 0.1249\n",
      "best loss:  0.12492638850798372\n",
      "Epoch [14/500] - Training Loss: 0.0970 - Val Loss: 0.1134\n",
      "best loss:  0.11341904030471552\n",
      "Epoch [15/500] - Training Loss: 0.0933 - Val Loss: 0.1058\n",
      "best loss:  0.10583771166742825\n",
      "Epoch [16/500] - Training Loss: 0.0825 - Val Loss: 0.1054\n",
      "best loss:  0.10544120013469556\n",
      "Epoch [17/500] - Training Loss: 0.0908 - Val Loss: 0.1120\n",
      "Epoch [18/500] - Training Loss: 0.0796 - Val Loss: 0.1094\n",
      "Epoch [19/500] - Training Loss: 0.0780 - Val Loss: 0.1025\n",
      "best loss:  0.10245832055807114\n",
      "Epoch [20/500] - Training Loss: 0.0667 - Val Loss: 0.1000\n",
      "best loss:  0.09999820767123191\n",
      "Epoch [21/500] - Training Loss: 0.0623 - Val Loss: 0.0979\n",
      "best loss:  0.09791110504846104\n",
      "Epoch [22/500] - Training Loss: 0.0781 - Val Loss: 0.1086\n",
      "Epoch [23/500] - Training Loss: 0.0703 - Val Loss: 0.0915\n",
      "best loss:  0.09149701404766958\n",
      "Epoch [24/500] - Training Loss: 0.0640 - Val Loss: 0.0955\n",
      "Epoch [25/500] - Training Loss: 0.0597 - Val Loss: 0.0895\n",
      "best loss:  0.0894802665857018\n",
      "Epoch [26/500] - Training Loss: 0.0505 - Val Loss: 0.0823\n",
      "best loss:  0.08227103985235339\n",
      "Epoch [27/500] - Training Loss: 0.0493 - Val Loss: 0.0759\n",
      "best loss:  0.07586292061405103\n",
      "Epoch [28/500] - Training Loss: 0.0585 - Val Loss: 0.1868\n",
      "Epoch [29/500] - Training Loss: 0.0731 - Val Loss: 0.0946\n",
      "Epoch [30/500] - Training Loss: 0.0566 - Val Loss: 0.1010\n",
      "Epoch [31/500] - Training Loss: 0.0530 - Val Loss: 0.1065\n",
      "Epoch [32/500] - Training Loss: 0.0582 - Val Loss: 0.0924\n",
      "Epoch [33/500] - Training Loss: 0.0506 - Val Loss: 0.0826\n",
      "Epoch [34/500] - Training Loss: 0.0474 - Val Loss: 0.0820\n",
      "Epoch [35/500] - Training Loss: 0.0461 - Val Loss: 0.0735\n",
      "best loss:  0.07347588845696605\n",
      "Epoch [36/500] - Training Loss: 0.0438 - Val Loss: 0.0815\n",
      "Epoch [37/500] - Training Loss: 0.0438 - Val Loss: 0.0792\n",
      "Epoch [38/500] - Training Loss: 0.0428 - Val Loss: 0.1246\n",
      "Epoch [39/500] - Training Loss: 0.0440 - Val Loss: 0.0708\n",
      "best loss:  0.0707976589070969\n",
      "Epoch [40/500] - Training Loss: 0.0382 - Val Loss: 0.0668\n",
      "best loss:  0.06681210263708576\n",
      "Epoch [41/500] - Training Loss: 0.0380 - Val Loss: 0.0886\n",
      "Epoch [42/500] - Training Loss: 0.0364 - Val Loss: 0.0708\n",
      "Epoch [43/500] - Training Loss: 0.0424 - Val Loss: 0.0742\n",
      "Epoch [44/500] - Training Loss: 0.0331 - Val Loss: 0.0799\n",
      "Epoch [45/500] - Training Loss: 0.0327 - Val Loss: 0.0946\n",
      "Epoch [46/500] - Training Loss: 0.0385 - Val Loss: 0.0945\n",
      "Epoch [47/500] - Training Loss: 0.0345 - Val Loss: 0.0760\n",
      "Epoch [48/500] - Training Loss: 0.0306 - Val Loss: 0.0838\n",
      "Epoch [49/500] - Training Loss: 0.0327 - Val Loss: 0.0758\n",
      "Epoch [50/500] - Training Loss: 0.0334 - Val Loss: 0.0724\n",
      "Epoch [51/500] - Training Loss: 0.0333 - Val Loss: 0.0897\n",
      "Early stopping triggered at epoch 51\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "vgg16_unet_model = smp.Unet(\n",
    "    encoder_name=\"vgg16\",     \n",
    "    encoder_weights=\"imagenet\",    \n",
    "    in_channels=3,                 \n",
    "    classes=1                    \n",
    ")\n",
    "vgg16_unet_model.to(device)\n",
    "train_model(vgg16_unet_model, cvc_train_loader, cvc_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.8361\n",
      "Average AUROC: 0.9913\n"
     ]
    }
   ],
   "source": [
    "inference(vgg16_unet_model, cvc_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = U_Net()\n",
    "model = model.to(device)\n",
    "train_model(model, kvasirinst_train_loader, kvasirinst_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, kvasirinst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "vgg16_unet_model = smp.Unet(\n",
    "    encoder_name=\"vgg16\",     \n",
    "    encoder_weights=\"imagenet\",    \n",
    "    in_channels=3,                 \n",
    "    classes=1                    \n",
    ")\n",
    "vgg16_unet_model.to(device)\n",
    "train_model(vgg16_unet_model, kvasirinst_train_loader, kvasirinst_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(vgg16_unet_model, kvasirinst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Training Loss: 0.6055 - Val Loss: 1.0405\n",
      "best loss:  1.040546452999115\n",
      "Epoch [2/500] - Training Loss: 0.5024 - Val Loss: 0.5007\n",
      "best loss:  0.5006719529628754\n",
      "Epoch [3/500] - Training Loss: 0.4781 - Val Loss: 0.4776\n",
      "best loss:  0.4776441127061844\n",
      "Epoch [4/500] - Training Loss: 0.4666 - Val Loss: 0.4640\n",
      "best loss:  0.46398368030786513\n",
      "Epoch [5/500] - Training Loss: 0.4483 - Val Loss: 0.4483\n",
      "best loss:  0.4482656195759773\n",
      "Epoch [6/500] - Training Loss: 0.4332 - Val Loss: 0.4350\n",
      "best loss:  0.43503053337335584\n",
      "Epoch [7/500] - Training Loss: 0.4256 - Val Loss: 0.4304\n",
      "best loss:  0.4304482087492943\n",
      "Epoch [8/500] - Training Loss: 0.4247 - Val Loss: 0.4221\n",
      "best loss:  0.42210519015789033\n",
      "Epoch [9/500] - Training Loss: 0.4096 - Val Loss: 0.4389\n",
      "Epoch [10/500] - Training Loss: 0.4056 - Val Loss: 0.5081\n",
      "Epoch [11/500] - Training Loss: 0.4000 - Val Loss: 0.3993\n",
      "best loss:  0.3992871791124344\n",
      "Epoch [12/500] - Training Loss: 0.3849 - Val Loss: 0.4041\n",
      "Epoch [13/500] - Training Loss: 0.3780 - Val Loss: 0.3534\n",
      "best loss:  0.3534269168972969\n",
      "Epoch [14/500] - Training Loss: 0.3613 - Val Loss: 0.3693\n",
      "Epoch [15/500] - Training Loss: 0.3514 - Val Loss: 0.3413\n",
      "best loss:  0.3413334682583809\n",
      "Epoch [16/500] - Training Loss: 0.3475 - Val Loss: 0.3485\n",
      "Epoch [17/500] - Training Loss: 0.3453 - Val Loss: 0.3409\n",
      "best loss:  0.3408633328974247\n",
      "Epoch [18/500] - Training Loss: 0.3320 - Val Loss: 0.3418\n",
      "Epoch [19/500] - Training Loss: 0.3305 - Val Loss: 0.3315\n",
      "best loss:  0.33151037618517876\n",
      "Epoch [20/500] - Training Loss: 0.3244 - Val Loss: 0.3190\n",
      "best loss:  0.31895776763558387\n",
      "Epoch [21/500] - Training Loss: 0.3182 - Val Loss: 0.3109\n",
      "best loss:  0.31088574081659315\n",
      "Epoch [22/500] - Training Loss: 0.3165 - Val Loss: 0.3477\n",
      "Epoch [23/500] - Training Loss: 0.3103 - Val Loss: 0.3254\n",
      "Epoch [24/500] - Training Loss: 0.3123 - Val Loss: 0.3083\n",
      "best loss:  0.30827592238783835\n",
      "Epoch [25/500] - Training Loss: 0.3014 - Val Loss: 0.2965\n",
      "best loss:  0.29652979001402857\n",
      "Epoch [26/500] - Training Loss: 0.2964 - Val Loss: 0.3106\n",
      "Epoch [27/500] - Training Loss: 0.2923 - Val Loss: 0.3174\n",
      "Epoch [28/500] - Training Loss: 0.2892 - Val Loss: 0.2926\n",
      "best loss:  0.2925731062889099\n",
      "Epoch [29/500] - Training Loss: 0.2908 - Val Loss: 0.3000\n",
      "Epoch [30/500] - Training Loss: 0.2823 - Val Loss: 0.2840\n",
      "best loss:  0.28401190489530564\n",
      "Epoch [31/500] - Training Loss: 0.2790 - Val Loss: 0.2788\n",
      "best loss:  0.2787752129137516\n",
      "Epoch [32/500] - Training Loss: 0.2712 - Val Loss: 0.2800\n",
      "Epoch [33/500] - Training Loss: 0.2614 - Val Loss: 0.2671\n",
      "best loss:  0.2671200029551983\n",
      "Epoch [34/500] - Training Loss: 0.2647 - Val Loss: 0.2595\n",
      "best loss:  0.2595421768724918\n",
      "Epoch [35/500] - Training Loss: 0.2547 - Val Loss: 0.2576\n",
      "best loss:  0.25757075697183607\n",
      "Epoch [36/500] - Training Loss: 0.2600 - Val Loss: 0.2560\n",
      "best loss:  0.25596628338098526\n",
      "Epoch [37/500] - Training Loss: 0.2464 - Val Loss: 0.2564\n",
      "Epoch [38/500] - Training Loss: 0.2437 - Val Loss: 0.2644\n",
      "Epoch [39/500] - Training Loss: 0.2425 - Val Loss: 0.2401\n",
      "best loss:  0.24009519666433335\n",
      "Epoch [40/500] - Training Loss: 0.2307 - Val Loss: 0.2487\n",
      "Epoch [41/500] - Training Loss: 0.2234 - Val Loss: 0.2409\n",
      "Epoch [42/500] - Training Loss: 0.2263 - Val Loss: 0.2286\n",
      "best loss:  0.22863527312874793\n",
      "Epoch [43/500] - Training Loss: 0.2221 - Val Loss: 0.2275\n",
      "best loss:  0.22750264257192612\n",
      "Epoch [44/500] - Training Loss: 0.2243 - Val Loss: 0.2176\n",
      "best loss:  0.21763618662953377\n",
      "Epoch [45/500] - Training Loss: 0.2225 - Val Loss: 0.2116\n",
      "best loss:  0.2115644708275795\n",
      "Epoch [46/500] - Training Loss: 0.2115 - Val Loss: 0.2064\n",
      "best loss:  0.2063503421843052\n",
      "Epoch [47/500] - Training Loss: 0.2041 - Val Loss: 0.2290\n",
      "Epoch [48/500] - Training Loss: 0.2042 - Val Loss: 0.2067\n",
      "Epoch [49/500] - Training Loss: 0.1993 - Val Loss: 0.2245\n",
      "Epoch [50/500] - Training Loss: 0.2000 - Val Loss: 0.2056\n",
      "best loss:  0.2056490946561098\n",
      "Epoch [51/500] - Training Loss: 0.1996 - Val Loss: 0.1921\n",
      "best loss:  0.19209676161408423\n",
      "Epoch [52/500] - Training Loss: 0.1951 - Val Loss: 0.2276\n",
      "Epoch [53/500] - Training Loss: 0.1955 - Val Loss: 0.1991\n",
      "Epoch [54/500] - Training Loss: 0.1889 - Val Loss: 0.1944\n",
      "Epoch [55/500] - Training Loss: 0.1897 - Val Loss: 0.2002\n",
      "Epoch [56/500] - Training Loss: 0.1772 - Val Loss: 0.1919\n",
      "best loss:  0.19192454665899278\n",
      "Epoch [57/500] - Training Loss: 0.1792 - Val Loss: 0.1882\n",
      "best loss:  0.18820845782756807\n",
      "Epoch [58/500] - Training Loss: 0.1861 - Val Loss: 0.1866\n",
      "best loss:  0.18662963807582855\n",
      "Epoch [59/500] - Training Loss: 0.1850 - Val Loss: 0.1911\n",
      "Epoch [60/500] - Training Loss: 0.1822 - Val Loss: 0.1872\n",
      "Epoch [61/500] - Training Loss: 0.1750 - Val Loss: 0.1917\n",
      "Epoch [62/500] - Training Loss: 0.1750 - Val Loss: 0.1846\n",
      "best loss:  0.18455015830695629\n",
      "Epoch [63/500] - Training Loss: 0.1788 - Val Loss: 0.1716\n",
      "best loss:  0.17158141434192659\n",
      "Epoch [64/500] - Training Loss: 0.1736 - Val Loss: 0.1920\n",
      "Epoch [65/500] - Training Loss: 0.1686 - Val Loss: 0.2049\n",
      "Epoch [66/500] - Training Loss: 0.1731 - Val Loss: 0.1935\n",
      "Epoch [67/500] - Training Loss: 0.1679 - Val Loss: 0.2154\n",
      "Epoch [68/500] - Training Loss: 0.1761 - Val Loss: 0.1923\n",
      "Epoch [69/500] - Training Loss: 0.1663 - Val Loss: 0.1759\n",
      "Epoch [70/500] - Training Loss: 0.1620 - Val Loss: 0.1712\n",
      "best loss:  0.17116059958934784\n",
      "Epoch [71/500] - Training Loss: 0.1612 - Val Loss: 0.1616\n",
      "best loss:  0.1616489801555872\n",
      "Epoch [72/500] - Training Loss: 0.1615 - Val Loss: 0.1847\n",
      "Epoch [73/500] - Training Loss: 0.1567 - Val Loss: 0.1925\n",
      "Epoch [74/500] - Training Loss: 0.1633 - Val Loss: 0.1798\n",
      "Epoch [75/500] - Training Loss: 0.1506 - Val Loss: 0.1870\n",
      "Epoch [76/500] - Training Loss: 0.1519 - Val Loss: 0.1840\n",
      "Epoch [77/500] - Training Loss: 0.1466 - Val Loss: 0.1744\n",
      "Epoch [78/500] - Training Loss: 0.1477 - Val Loss: 0.1668\n",
      "Epoch [79/500] - Training Loss: 0.1503 - Val Loss: 0.1789\n",
      "Epoch [80/500] - Training Loss: 0.1523 - Val Loss: 0.1794\n",
      "Epoch [81/500] - Training Loss: 0.1486 - Val Loss: 0.1867\n",
      "Epoch [82/500] - Training Loss: 0.1401 - Val Loss: 0.1647\n",
      "Early stopping triggered at epoch 82\n"
     ]
    }
   ],
   "source": [
    "model = U_Net()\n",
    "model = model.to(device)\n",
    "train_model(model, kvasirseg_train_loader, kvasirseg_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.6995\n",
      "Average AUROC: 0.9697\n"
     ]
    }
   ],
   "source": [
    "inference(model, kvasirseg_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Training Loss: 0.3928 - Val Loss: 0.2873\n",
      "best loss:  0.2872691288590431\n",
      "Epoch [2/500] - Training Loss: 0.2629 - Val Loss: 0.2458\n",
      "best loss:  0.24583308771252632\n",
      "Epoch [3/500] - Training Loss: 0.2158 - Val Loss: 0.2030\n",
      "best loss:  0.20301557704806328\n",
      "Epoch [4/500] - Training Loss: 0.1893 - Val Loss: 0.1997\n",
      "best loss:  0.19974569380283355\n",
      "Epoch [5/500] - Training Loss: 0.1749 - Val Loss: 0.1481\n",
      "best loss:  0.14809590466320516\n",
      "Epoch [6/500] - Training Loss: 0.1495 - Val Loss: 0.1681\n",
      "Epoch [7/500] - Training Loss: 0.1579 - Val Loss: 0.1568\n",
      "Epoch [8/500] - Training Loss: 0.1361 - Val Loss: 0.1558\n",
      "Epoch [9/500] - Training Loss: 0.1312 - Val Loss: 0.1394\n",
      "best loss:  0.13944493941962718\n",
      "Epoch [10/500] - Training Loss: 0.1218 - Val Loss: 0.1553\n",
      "Epoch [11/500] - Training Loss: 0.1243 - Val Loss: 0.1336\n",
      "best loss:  0.13364214599132537\n",
      "Epoch [12/500] - Training Loss: 0.1129 - Val Loss: 0.1304\n",
      "best loss:  0.13044996820390226\n",
      "Epoch [13/500] - Training Loss: 0.1099 - Val Loss: 0.1520\n",
      "Epoch [14/500] - Training Loss: 0.1074 - Val Loss: 0.1422\n",
      "Epoch [15/500] - Training Loss: 0.1029 - Val Loss: 0.1181\n",
      "best loss:  0.11807781830430031\n",
      "Epoch [16/500] - Training Loss: 0.1022 - Val Loss: 0.1221\n",
      "Epoch [17/500] - Training Loss: 0.1019 - Val Loss: 0.1271\n",
      "Epoch [18/500] - Training Loss: 0.0976 - Val Loss: 0.1409\n",
      "Epoch [19/500] - Training Loss: 0.0976 - Val Loss: 0.2371\n",
      "Epoch [20/500] - Training Loss: 0.0826 - Val Loss: 0.2483\n",
      "Epoch [21/500] - Training Loss: 0.0946 - Val Loss: 0.1208\n",
      "Epoch [22/500] - Training Loss: 0.0807 - Val Loss: 0.1176\n",
      "best loss:  0.11755742430686951\n",
      "Epoch [23/500] - Training Loss: 0.0751 - Val Loss: 0.1071\n",
      "best loss:  0.10705883055925369\n",
      "Epoch [24/500] - Training Loss: 0.0761 - Val Loss: 0.1282\n",
      "Epoch [25/500] - Training Loss: 0.0771 - Val Loss: 0.1272\n",
      "Epoch [26/500] - Training Loss: 0.0783 - Val Loss: 0.1157\n",
      "Epoch [27/500] - Training Loss: 0.0780 - Val Loss: 0.1202\n",
      "Epoch [28/500] - Training Loss: 0.0756 - Val Loss: 0.1082\n",
      "Epoch [29/500] - Training Loss: 0.0679 - Val Loss: 0.1233\n",
      "Epoch [30/500] - Training Loss: 0.0639 - Val Loss: 0.1326\n",
      "Epoch [31/500] - Training Loss: 0.0626 - Val Loss: 0.1241\n",
      "Epoch [32/500] - Training Loss: 0.0777 - Val Loss: 0.1252\n",
      "Epoch [33/500] - Training Loss: 0.0646 - Val Loss: 0.1052\n",
      "best loss:  0.10519897993654012\n",
      "Epoch [34/500] - Training Loss: 0.0724 - Val Loss: 0.1377\n",
      "Epoch [35/500] - Training Loss: 0.0563 - Val Loss: 0.1322\n",
      "Epoch [36/500] - Training Loss: 0.0592 - Val Loss: 0.1174\n",
      "Epoch [37/500] - Training Loss: 0.0559 - Val Loss: 0.1374\n",
      "Epoch [38/500] - Training Loss: 0.0598 - Val Loss: 0.1340\n",
      "Epoch [39/500] - Training Loss: 0.0592 - Val Loss: 0.1106\n",
      "Epoch [40/500] - Training Loss: 0.0586 - Val Loss: 0.1138\n",
      "Epoch [41/500] - Training Loss: 0.0575 - Val Loss: 0.1210\n",
      "Epoch [42/500] - Training Loss: 0.0625 - Val Loss: 0.1267\n",
      "Epoch [43/500] - Training Loss: 0.0524 - Val Loss: 0.1117\n",
      "Epoch [44/500] - Training Loss: 0.0492 - Val Loss: 0.1055\n",
      "Early stopping triggered at epoch 44\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "vgg16_unet_model = smp.Unet(\n",
    "    encoder_name=\"vgg16\",     \n",
    "    encoder_weights=\"imagenet\",    \n",
    "    in_channels=3,                 \n",
    "    classes=1                    \n",
    ")\n",
    "vgg16_unet_model.to(device)\n",
    "train_model(vgg16_unet_model, kvasirseg_train_loader, kvasirseg_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average IoU: 0.8014\n",
      "Average AUROC: 0.9904\n"
     ]
    }
   ],
   "source": [
    "inference(vgg16_unet_model, kvasirseg_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m U_Net()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misic_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misic_val_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[1;32m     17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[21], line 63\u001b[0m, in \u001b[0;36mSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m     61\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths[idx]\n\u001b[0;32m---> 63\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     64\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/PIL/Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 995\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/unet/lib/python3.11/site-packages/PIL/ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = U_Net()\n",
    "model = model.to(device)\n",
    "train_model(model, isic_train_loader, isic_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(model, isic_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "vgg16_unet_isic_model = smp.Unet(\n",
    "    encoder_name=\"vgg16\",     \n",
    "    encoder_weights=\"imagenet\",    \n",
    "    in_channels=3,                 \n",
    "    classes=1                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_unet_isic_model.to(device)\n",
    "train_model(vgg16_unet_isic_model, isic_train_loader, isic_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(vgg16_unet_isic_model, isic_test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
